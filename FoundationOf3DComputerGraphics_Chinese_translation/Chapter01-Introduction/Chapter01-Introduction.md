# 介绍
计算机图形是一个神奇的技术成功故事。基础思路，表达，算法和硬件方式在19世纪60年代和70年代被锻造出来，并且在随后的20年内发展成熟。到90年代中期，计算机图形技术已经相当成熟，但是他们的影响力在某种程度上被限制在“高端”应用，诸如超级计算机上的科学可视化，以及昂贵的飞行模拟器。现在很难相信，但是很多本科毕业的计算机专业的学习甚至对于3D计算机图形是什么都没有概念！

之前的10年见证了计算机图形技术的大规模商业化。每一台现代PC都可以产生高质量的计算机生成的图像，大部分是以视频游戏和虚拟生活环境的形式存在。整个动画工业已经从高端（比如，皮克斯电影）转换到了儿童的日常电视节目中。对于真人实拍的电影，特性领域已经被完全革新；今天的观看者在他们看到最令人吃惊的计算机生成的特效时不会大惊小怪，这完全是期望看到的。

## 1.1 OpenGL
OpenGL开始时作为一种API，用于实现3D计算机图形的一种非常专有的操作序列。当底层的硬件变得越来越便宜，越来越多的弹性被放入硬件中并且通过OpenGL API被暴漏给了用户。随着时间的推移，完全控制图形计算的某一部分就变得可行。用户通过写小的专有目的程序，称为着色器（shader），这些程序被传递给API并且被其编译。在OpenGL中，这些着色器以类C的专有目的语言（称作GLSL）来书写记录。就像我们下面将描述的，两个主要的可编程部分分别一个顶点着色器（vertex shader）和一个碎片着色器（fragment shader）所控制。这些特定的部分之所以被实现为可编程的方式，因为它们本身不仅在给予用户弹性方面有巨大效用，而且也因为这种计算可以借助单指令多数据（SIMD）并行方式被实现。以单个几何顶点方式被存储的数据可以独立于其它顶点被处理。同样的，确定一个屏幕像素色彩的计算可以完全独立于其它像素被计算。

在当前的OpenGL程序中，实际的3D图形很大程度（但不是全部）由你所写的着色器完成，不再是OpenGL API本身的真正组成部分。在这种意义上，OpenGL更多的是关于组织你的数据和着色器而更少关于3D计算机图形。在这部分的余下内容里，我们会给出由OpenGL完成的主要处理步骤的概要。但是我们也会给出多种关于着色器通常是怎么被用于这些步骤中实现来3D计算机图形的高级描述。


In OpenGL we represent our geometry as a collection of triangles. On the one hand, triangles are simple enough to be processed very efﬁciently by OpenGL, while on the other hand, using collections of many triangles, we can approximate surfaces with complicated shapes (see Figure 1.1). If our computer graphics program uses a more abstract geometric representation, it must ﬁrst be turned into a triangle collection before OpenGL can draw the geometry.

Brieﬂy stated, the computation in OpenGL determines the screen position for each vertex of each of the triangles, ﬁgures out which screen dots, called pixels, lie within each triangle, and then performs some computation to determine the desired color of that pixel. We now walk through these steps in a bit more detail.

Each triangle is made up of 3 vertices. We associate some numerical data with each vertex. Each such data item is called an attribute. At the very least, we need to specify the location of the vertex (using 2 numbers for 2D geometry or 3 numbers for 3D geometry). We can use other attributes to associate other kinds of data with our vertices that we will use to determine their ultimate appearances. For example, we may associate a color (using 3 numbers representing amounts of red, green and blue) with each vertex. Other attributes might be used to represent relevant material properties describing, say, how shiny the surface at the vertex is.

Transmitting the vertex data from the CPU to the graphics hardware (the GPU) is an expensive process, so it is typically done as infrequently as possible. There are speciﬁc API calls to transfer vertex data over to OpenGL which stores this data in a vertex buffer

Once the vertex data has been given to OpenGL, at any subsequent time we can send a draw call to OpenGL. This commands OpenGL to walk down the appropriate vertex buffers and draw each vertex triplet as a triangle.

Once the OpenGL draw call has been issued, each vertex (i.e., all of its attributes) gets processed independently by your vertex shader (See Figure 1.2). Besides the attribute data, the shader also has access to things called uniform variables. These are variables that are set by your program, but you can only set them in between OpenGL draw calls, and not per vertex.

The vertex shader is your own program, and you can put whatever you want in it. The most typical use of the vertex shader is to determine the ﬁnal position of the vertices on the screen. For example, a vertex can have its own abstract 3D position stored as an attribute. Meanwhile, a uniform variable can be used to describe a virtual camera that maps abstract 3D coordinates to the actual 2D screen. We will cover the details of this kind of computation in Chapters 2- 6 and Chapter 10.

Once the vertex shader has computed the ﬁnal position of the vertex on the screen, it assigns this value to the the reserved output variable called gl Position. The x and y coordinates of this variable are interpreted as positions within the drawing window. The lower left corner of the window has coordinates (−1, −1), and the upper right corner has coordinates (1, 1). Coordinates outside of this square represent locations outside of the drawing area.

The vertex shader can also output other variables that will be used by the fragment shader to determine the ﬁnal color of each pixel covered by the triangle. These outputs are called varying variables since as we will soon explain, their values can vary as we look at the different pixels within a triangle.

Once processed, these vertices along with their varying variables are collected by the triangle assembler, and grouped together in triplets.

OpenGL’s next job is to draw each triangle on the screen (see Figure 1.3). This step is called rasterization. For each triangle, it uses the three vertex positions to place the triangle on the screen. It then computes which of the pixels on the screen are inside of this triangle. For each such pixel, the rasterizer computes an interpolated value for each of the varying variables. This means that the value for each varying variable is set by blending the three values associated with the triangle’s vertices. The blending ratios used are related to the pixel’s distance to each of the three vertices. We will cover the exact method of blending in Chapter 13. Because rasterization is such a specialized and highly optimized operation, this step has not been made programmable.

Finally, for each pixel, this interpolated data is passed through a fragment shader (see Figure 1.4). A fragment shader is another program that you write in the GLSL language and hand off to OpenGL. The job of the fragment shader is to determine the drawn color of the pixel based on the information passed to it as varying and uniform variables. This ﬁnal color computed by the fragment shader is placed in a part of GPU memory called a framebuffer. The data in the framebuffer is then sent to the display, where it is drawn on the screen.

In 3D graphics, we typically determine a pixel’s color by computing a few equations that simulate the way that light reﬂects off of some material’s surface. This calculation may use data stored in varying variables that represent the material and geometric properties of the material at that pixel. It may also use data stored in uniform variables that represent the position and color of the light sources in the scene. By changing the program in the fragment shader, we can simulate light bouncing off of different types of materials, this can create a variety of appearances for some ﬁxed geometry, as shown in Figure 1.5. We discuss this process in greater detail in Chapter 14.

As part of this color computation, we can also instruct fragment shader to fetch color data from an auxiliary stored image. Such an image is called a texture and is pointed to by a uniform variable. Meanwhile, varying variables called texture coordinates tell the fragment shader where to select the appropriate pixels from the texture. Using this process called texture mapping, one can simulate the “gluing” of a some part of a texture image onto each triangle. This process can be used to give high visual complexity to a simple geometric object deﬁned by only a small number of triangles. See Figure 1.6 for such an example. This is discussed further in Chapter 15.

When colors are drawn to the framebuffer, there is a process called merging which determines how the “new” color that has just been output from the fragment shader is mixed in with the “old” color that may already exist in the framebuffer. When zbuffering is enabled a test is applied to see whether the geometric point just processed by the fragment shader is closer to, or farther from the viewer, than the point that was used to set the existing color in the framebuffer. The framebuffer is then updated only if the new point is closer. Z-buffering is very useful in creating images of 3D scenes. We discuss z-buffering in Chapter 11. In addition, OpenGL can also be instructed to blend the old and new colors together using various ratios. This can be used for example to model transparent objects. This process is called alpha blending and is discussed further in Section 16.4. Because this merging step involves reading and writing to shared memory (the framebuffer), this step has not been made programmable, but is instead controlled by various API calls.

In Appendix A we walk through an actual code fragment that implements a simple OpenGL program that performs some simple 2D drawing with texture mapping. The goal there is not to learn 3D graphics, but to get an understanding of the API itself and the processing steps used in OpenGL. You will need to go through this Appendix in detail at some point before you get to Chapter 6.

